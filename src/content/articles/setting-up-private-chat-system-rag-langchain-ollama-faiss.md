---
title: "Setting Up a Private Chat System (RAG) with LangChain, Ollama, and FAISS Vector Store: A Step-by-Step Guide"
path: "/articles/setting-up-private-chat-system-rag-langchain-ollama-faiss/"
date: 2025-03-20
last_modified_at: 2025-03-20T12:00:00-05:00
excerpt: "In this post, I'll walk you through the process of building a sophisticated chat conversation system using Retrieval-Augmented Generation (RAG). This system leverages LangChain to orchestrate components, OLLAMA's LLaMA3.2:1B for language modeling, and FAISS for vector similarity search."
image: ""
categories: ["AI", "Machine Learning", "Tutorial", "RAG"]
tags: ["AI", "Langchain", "Chatbots", "Ollama", "How To", "RAG", "FAISS", "Vector Store", "Python"]
toc: true
featured: false
draft: false
---

# Setting Up a Private Chat System (RAG) with LangChain, Ollama, and FAISS Vector Store: A Step-by-Step Guide

In this post, I'll walk you through the process of building a sophisticated chat conversation system using Retrieval-Augmented Generation (RAG). This system leverages LangChain to orchestrate components, OLLAMA's LLaMA3.2:1B for language modeling, and FAISS for vector similarity search — all tied together with a modular, Python-based code architecture. Let's dive into the project overview, technical details, and step-by-step implementation.

## Project Overview

The goal was to create a chatbot that can intelligently process user queries by retrieving relevant information from a preprocessed PDF document. Key components include:

- **PDF Data Extraction**: Using PyMuPDF (fitz) to extract text from a research paper.
- **Text Processing and Embeddings**: Splitting the extracted text into manageable chunks, then generating high-quality embeddings with Ollama's nomic-embed-text model.
- **Vector Database Storage**: Storing the embeddings in FAISS for efficient similarity search.
- **LangChain Integration**: Creating a retrieval chain and designing prompt templates that guide the LLM to produce context-aware responses.
- **Testing and Refinement**: Evaluating chatbot performance through questions generated by ChatGPT and refining prompt templates based on feedback.

## System Architecture

The project is organized into three major phases:

![](https://miro.medium.com/v2/resize:fit:1400/1*jlQEOSwgaBseXz_B-2r70w.png)

## 1. Data Extraction and Embedding Creation

- **PDF Extraction**: The first step involves extracting text from a research paper PDF using PyMuPDF. This tool efficiently navigates through PDF pages, ensuring that all relevant text is captured.
- **Text Splitting**: Once the text is extracted, it is split into smaller chunks. This is essential for two reasons: it improves the quality of embeddings and ensures that each segment is semantically coherent for later retrieval.
- **Creating Embeddings**: Using Ollama's nomic-embed-text model, each text chunk is transformed into a high-quality vector representation. These embeddings capture the semantic meaning of the text.
- **Storing Embeddings in FAISS**: The generated embeddings are stored in FAISS (Facebook AI Similarity Search), which is an efficient vector database that allows for fast similarity searches.

## 2. LangChain Chain and Prompt Engineering

- **Retrieval Chain Setup**: LangChain is configured to create a retrieval chain that efficiently fetches the most relevant chunks from the FAISS database based on the user's query.
- **Prompt Template Design**: A well-structured prompt template is crafted specifically for the Llama LLM. This template provides the necessary context and query details to guide the model's response generation.
- **Integration**: The retrieval chain, prompt template, and Llama model are integrated seamlessly to enable smooth interaction between the user input, context retrieval, and final response generation.

## 3. Testing, Refinement, and Evaluation

- **Testing Methodology**: To ensure unbiased evaluation, the chatbot is tested with questions generated by ChatGPT — questions that were not part of the training data.
- **Response Evaluation**: After responses are generated, they are compared with expected outputs. This step is crucial for identifying any gaps or inaccuracies in the chatbot's replies.
- **Prompt Optimization**: Based on the evaluation, the prompt structure and retrieval process are refined to enhance the chatbot's accuracy, coherence, and relevance.

![](https://miro.medium.com/v2/resize:fit:1400/1*WhIpFujjtP3lY4lPWIWhcA.png)

## Step 1: Environment Setup

Before writing any code, it's essential to set up the environment. This involves creating a virtual environment and installing all the required dependencies. Below is an example shell command sequence:

```bash
# Create a virtual environment
python -m venv ml

# Activate the environment (PowerShell)
.\ml\Scripts\Activate.ps1

# If you run into script execution issues, resolve them with:
Set-ExecutionPolicy Unrestricted -Scope Process
```

```bash
# Install the required dependencies

pip install -U langchain-community
pip install -U faiss-cpu
pip install -U langchain-huggingface
pip install -U pymupdf
pip install -U tiktoken
pip install -U langchain-ollama
pip install -U python-dotenv
```

### Explanation

- **Virtual Environment**: Isolates your project's dependencies.
- **Dependencies**: Each package plays a role — from extracting PDF text (`pymupdf`) to building the chatbot (`langchain-community`, `langchain-ollama`).

## Step 2: PDF Extraction and Text Preprocessing

The first code snippet extracts text from the PDF using PyMuPDF and splits the text into smaller chunks. This is important for creating quality embeddings later on.

```python
import fitz  # PyMuPDF
def extract_text_from_pdf(pdf_path):
    document = fitz.open(pdf_path)
    text = ""
    for page in document:
        text += page.get_text()
    return text

pdf_path = "path/to/your/research_paper.pdf"
document_text = extract_text_from_pdf(pdf_path)
print("Extracted Text Preview:", document_text[:500])
```

### Explanation

- **PyMuPDF (fitz)**: Opens and reads the PDF file.
- **Text Extraction**: Loops through each page to compile the text.
- **Preview**: Prints a snippet to verify the extraction.

Next, the text is split into manageable chunks for better embedding generation:

```python
def split_text(text, chunk_size=500, overlap=50):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
    return chunks

chunks = split_text(document_text)
print("Total chunks created:", len(chunks))
```

### Explanation

- **Chunking**: Splits the text into segments of 500 words with a small overlap (50 words) to maintain context.
- **Usage**: These chunks will be fed into the embedding model.

## Step 3: Embedding Generation and Storage with FAISS

In this stage, each text chunk is converted into a vector embedding using Ollama's embedding model. These embeddings are then stored in FAISS for efficient similarity searches.

```python
from langchain_ollama.embeddings import OllamaEmbedder  # hypothetical import based on your integration
import faiss
import numpy as np

# Create embeddings for each chunk
embedder = OllamaEmbedder(model_name="nomic-embed-text")
embeddings = [embedder.embed(chunk) for chunk in chunks]

# Convert list of embeddings to a NumPy array
embedding_dim = len(embeddings[0])
embeddings_np = np.array(embeddings).astype("float32")

# Build a FAISS index for similarity search
index = faiss.IndexFlatL2(embedding_dim)
index.add(embeddings_np)
print("FAISS index built with", index.ntotal, "embeddings")
```

### Explanation

- **Embedding Model**: Uses OLLAMA's `nomic-embed-text` to convert text into vectors.
- **FAISS**: A fast library for similarity search; we create an index using the L2 distance metric.
- **Embedding Storage**: The embeddings are stored in the FAISS index for rapid retrieval during query time.

## Step 4: LangChain Setup and Prompt Engineering

This part involves creating a retrieval chain using LangChain and designing a prompt template for generating responses with the LLM.

```python
from langchain import PromptTemplate, LLMChain
from langchain.retrievers import FAISSRetriever

# Define the prompt template
prompt_template = """
You are an intelligent assistant. Given the following context, answer the user query accurately:
Context: {context}
User Query: {query}
Answer:
"""

template = PromptTemplate(template=prompt_template, input_variables=["context", "query"])

# Set up the retriever using the FAISS index
retriever = FAISSRetriever(index=index, embeddings=embeddings_np, chunks=chunks, top_k=5)

# Define the LLM chain
llama_chain = LLMChain(llm="ollama_llama", prompt=template)

def get_response(user_query):
    # Retrieve relevant context chunks
    context_chunks = retriever.get_relevant_chunks(user_query)
    context = "\n".join(context_chunks)

    # Generate response using the chain
    answer = llama_chain.run({"context": context, "query": user_query})
    return answer

# Example query
user_query = "What are the key findings of the research paper?"
response = get_response(user_query)
print("Chatbot Response:", response)
```

### Explanation

- **Prompt Template**: Combines retrieved context with the user's query to guide the LLM.
- **FAISS Retriever**: Searches for the most relevant text chunks based on the user query.
- **LLM Chain**: Uses the prompt template and an LLM (via OLLAMA) to generate a final answer.
- **Integration**: `get_response` function encapsulates the entire process from retrieval to response generation.

## Step 5: Testing and Iteration

Finally, after setting up the chatbot, it's essential to test and refine it. In the notebook, I used various user queries to evaluate the performance. Here's a snippet that simulates testing:

```python
# List of test queries
test_queries = [
    "Summarize the data extraction process.",
    "How does the chatbot use FAISS?",
    "What improvements were made in prompt engineering?"
]
for query in test_queries:
    print("\nUser Query:", query)
    print("Response:", get_response(query))
```

### Explanation

- **Testing**: A series of test queries helps verify that the system retrieves relevant context and generates accurate responses.
- **Iteration**: Based on the responses, adjustments to the prompt template or retrieval parameters can be made for further improvements.

**Response from chatbot:**

![](https://miro.medium.com/v2/resize:fit:1400/1*zM74vnosEtKNV31Df4KtQg.png)

## Conclusion

This project demonstrates a comprehensive approach to building a retrieval-augmented chatbot by integrating several modern technologies:

- **Data Extraction**: Using PyMuPDF to extract and preprocess data from a PDF.
- **Embedding and Retrieval**: Converting text into embeddings and storing them in a FAISS index for efficient semantic search.
- **LLM Integration**: Utilizing LangChain and OLLAMA's LLaMA model to generate detailed, context-aware responses.
- **Iterative Testing**: Continuously refining the system based on user queries and feedback.

By following these steps and using the provided code snippets, you can build a similar system tailored to your own projects. Experiment with different parameters, try alternative vector databases, or adjust the prompt templates to see how they affect performance. Happy coding!

